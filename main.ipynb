{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec25680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8ceb0",
   "metadata": {},
   "source": [
    "Ucitavanje dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b7500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('text8') as f:\n",
    "    all_words = f.read().split()\n",
    "    sample = all_words[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa842e2",
   "metadata": {},
   "source": [
    "Uzimanje privh 1000 reci kao sample, \n",
    "Pravljenje vokabulara i njegovog inverza.\n",
    "{'anarchism': 0, 'originated': 1...}\n",
    "{0: 'anarchism', 1: 'originated'...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687c4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "count = collections.Counter(sample)\n",
    "\n",
    "sorted_count = count.most_common()\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "inverse_vocab = {}\n",
    "\n",
    "for word, _ in sorted_count:\n",
    "    vocab[word] = len(vocab)\n",
    "    inverse_vocab[len(inverse_vocab)] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a15afa",
   "metadata": {},
   "source": [
    "Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4afdc666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originalna dužina: 50000\n",
      "Nakon subsamplinga: 22764\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "t = 1e-4  # Threshold \n",
    "total_count = len(sample)\n",
    "word_freqs = {word: count[word] / total_count for word in vocab}\n",
    "\n",
    "subsampled_sample = []\n",
    "\n",
    "for word in sample:\n",
    "    f_wi = word_freqs[word]\n",
    "    # P(wi) = 1 - sqrt(t / f(wi))\n",
    "    p_discard = max(0, 1 - np.sqrt(t / f_wi))\n",
    "    \n",
    "    # Ako je nasumicni broj veci od verovatnoee odbacivanja, zadrzavamo rec\n",
    "    if random.random() > p_discard:\n",
    "        subsampled_sample.append(word)\n",
    "\n",
    "print(f\"Originalna dužina: {len(sample)}\")\n",
    "print(f\"Nakon subsamplinga: {len(subsampled_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579ac47",
   "metadata": {},
   "source": [
    "Pravljenje windowa(indeksiranje, tj. pravljenje parova po ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0c666e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukupno parova: 364152\n",
      "Prvih 20 parova (ID, ID): [[2470    5]\n",
      " [2470 1869]\n",
      " [2470  112]\n",
      " [2470  115]\n",
      " [2470 3764]\n",
      " [2470    0]\n",
      " [2470 2471]\n",
      " [2470  252]\n",
      " [   5 2470]\n",
      " [   5 1869]\n",
      " [   5  112]\n",
      " [   5  115]\n",
      " [   5 3764]\n",
      " [   5    0]\n",
      " [   5 2471]\n",
      " [   5  252]\n",
      " [   5 3765]\n",
      " [1869 2470]\n",
      " [1869    5]\n",
      " [1869  112]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 8\n",
    "training_pairs = []\n",
    "\n",
    "for i in range(len(subsampled_sample)):\n",
    "    #uzimamo id reci\n",
    "    center_word_id = vocab[subsampled_sample[i]]\n",
    "\n",
    "    start = max(0, i - window_size)\n",
    "    end = min(i + window_size + 1, len(subsampled_sample))\n",
    "\n",
    "    for j in range(start, end):\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        context_word_id = vocab[subsampled_sample[j]]\n",
    "        training_pairs.append((center_word_id, context_word_id))\n",
    "\n",
    "training_pairs = np.array(training_pairs)\n",
    "\n",
    "print(f\"Ukupno parova: {len(training_pairs)}\")\n",
    "print(f\"Prvih 20 parova (ID, ID): {training_pairs[:20]}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83ac2f",
   "metadata": {},
   "source": [
    "Pravljenje matrica w1 i w2, i postavljanje limita putem xavier inicijalizacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d281646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027695978881934938\n",
      "W1 (Vokabular x Vektor): (7792, 30)\n",
      "W2 (Vektor x Vokabular): (7792, 30)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vector_size = 30      \n",
    "batch_size = 512      \n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "num_neg_samples = 8 \n",
    "\n",
    "limit = np.sqrt(6 / (vocab_size + vector_size))\n",
    "\n",
    "W1 = np.random.uniform(-limit, limit, (vocab_size, vector_size))\n",
    "W2 = np.random.uniform(-limit, limit, (vocab_size, vector_size))\n",
    "\n",
    "\n",
    "print(limit)\n",
    "print(f\"W1 (Vokabular x Vektor): {W1.shape}\")\n",
    "print(f\"W2 (Vektor x Vokabular): {W2.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3423b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tabela generisana. Veličina: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Tabela veličine 100 miliona elemenata je standard, \n",
    "# ali za tvoj sample može i 1 milion da uštediš RAM\n",
    "table_size = 10**6 \n",
    "word_counts_arr = np.array([count[inverse_vocab[i]] for i in range(vocab_size)])\n",
    "probs = word_counts_arr**0.75\n",
    "probs /= probs.sum()\n",
    "\n",
    "# Generisanje tabele: svaka reč se pojavljuje srazmerno svojoj verovatnoći\n",
    "unigram_table = np.zeros(table_size, dtype=np.int32)\n",
    "current_pos = 0\n",
    "\n",
    "for word_id, p in enumerate(probs):\n",
    "    num_entries = int(p * table_size)\n",
    "    unigram_table[current_pos : current_pos + num_entries] = word_id\n",
    "    current_pos += num_entries\n",
    "\n",
    "# Popunjavamo ostatak (zbog zaokruživanja) poslednjom reči\n",
    "if current_pos < table_size:\n",
    "    unigram_table[current_pos:] = vocab_size - 1\n",
    "\n",
    "print(f\"Unigram tabela generisana. Veličina: {unigram_table.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f87bf50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Avg Loss: 3.241590\n",
      "Epoch 2/20 | Avg Loss: 2.999230\n",
      "Epoch 3/20 | Avg Loss: 2.823580\n",
      "Epoch 4/20 | Avg Loss: 2.741898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     77\u001b[39m     m_hat_W2_neg = m_W2[n_ids] / (\u001b[32m1\u001b[39m - beta1**t_step)\n\u001b[32m     78\u001b[39m     v_hat_W2_neg = v_W2[n_ids] / (\u001b[32m1\u001b[39m - beta2**t_step)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     W2_neg_update = -learning_rate * m_hat_W2_neg / (\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_hat_W2_neg\u001b[49m\u001b[43m)\u001b[49m + epsilon)\n\u001b[32m     81\u001b[39m     np.add.at(W2, n_ids, W2_neg_update)\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Avg Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/\u001b[38;5;28mlen\u001b[39m(training_pairs)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- INICIJALIZACIJA ADAMA (Uradi ovo pre petlje) ---\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "t_step = 0  # Brojač koraka za bias korekciju\n",
    "\n",
    "# Momenti za W1 i W2 (istih dimenzija kao matrice)\n",
    "m_W1, v_W1 = np.zeros_like(W1), np.zeros_like(W1)\n",
    "m_W2, v_W2 = np.zeros_like(W2), np.zeros_like(W2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -20, 20)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(training_pairs)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, len(training_pairs), batch_size):\n",
    "        batch = training_pairs[i : i + batch_size]\n",
    "        curr_batch = len(batch)\n",
    "        t_step += 1  # Povećavamo globalni brojač koraka\n",
    "        \n",
    "        c_ids = batch[:, 0] # Centralne reči\n",
    "        t_ids = batch[:, 1] # Target (pozitivne) reči\n",
    "        \n",
    "        # Negativni uzorci pomoću unigram tabele\n",
    "        random_indices = np.random.randint(0, table_size, size=(curr_batch, num_neg_samples))\n",
    "        n_ids = unigram_table[random_indices]\n",
    "        \n",
    "        # --- FORWARD ---\n",
    "        h = W1[c_ids]            \n",
    "        v_pos = W2[t_ids]        \n",
    "        v_neg = W2[n_ids]        \n",
    "        \n",
    "        pos_dot = np.sum(h * v_pos, axis=1)\n",
    "        pos_probs = sigmoid(pos_dot)\n",
    "        \n",
    "        neg_dot = np.einsum('ij,ikj->ik', h, v_neg) \n",
    "        neg_probs = sigmoid(-neg_dot)\n",
    "        \n",
    "        loss = -np.sum(np.log(pos_probs + 1e-10)) - np.sum(np.log(neg_probs + 1e-10))\n",
    "        total_loss += loss\n",
    "        \n",
    "        # --- BACKPROP (Gradijenti) ---\n",
    "        grad_pos = (pos_probs - 1).reshape(-1, 1) \n",
    "        grad_neg = (1 - neg_probs)                \n",
    "        \n",
    "        dW2_pos = grad_pos * h\n",
    "        dW2_neg = np.einsum('ik,ij->ikj', grad_neg, h)\n",
    "        dW1 = grad_pos * v_pos + np.einsum('ik,ikj->ij', grad_neg, v_neg)\n",
    "        \n",
    "        # --- ADAM UPDATE ZA W1 (Centralne reči) ---\n",
    "        # Napomena: Ažuriramo samo indekse koji su učestvovali u batch-u\n",
    "        m_W1[c_ids] = beta1 * m_W1[c_ids] + (1 - beta1) * dW1\n",
    "        v_W1[c_ids] = beta2 * v_W1[c_ids] + (1 - beta2) * (dW1**2)\n",
    "        \n",
    "        m_hat_W1 = m_W1[c_ids] / (1 - beta1**t_step)\n",
    "        v_hat_W1 = v_W1[c_ids] / (1 - beta2**t_step)\n",
    "        \n",
    "        W1_update = -learning_rate * m_hat_W1 / (np.sqrt(v_hat_W1) + epsilon)\n",
    "        np.add.at(W1, c_ids, W1_update)\n",
    "\n",
    "        # --- ADAM UPDATE ZA W2 (Pozitivni targeti) ---\n",
    "        m_W2[t_ids] = beta1 * m_W2[t_ids] + (1 - beta1) * dW2_pos\n",
    "        v_W2[t_ids] = beta2 * v_W2[t_ids] + (1 - beta2) * (dW2_pos**2)\n",
    "        \n",
    "        m_hat_W2_pos = m_W2[t_ids] / (1 - beta1**t_step)\n",
    "        v_hat_W2_pos = v_W2[t_ids] / (1 - beta2**t_step)\n",
    "        \n",
    "        W2_pos_update = -learning_rate * m_hat_W2_pos / (np.sqrt(v_hat_W2_pos) + epsilon)\n",
    "        np.add.at(W2, t_ids, W2_pos_update)\n",
    "\n",
    "        # --- ADAM UPDATE ZA W2 (Negativni targeti) ---\n",
    "        m_W2[n_ids] = beta1 * m_W2[n_ids] + (1 - beta1) * dW2_neg\n",
    "        v_W2[n_ids] = beta2 * v_W2[n_ids] + (1 - beta2) * (dW2_neg**2)\n",
    "        \n",
    "        m_hat_W2_neg = m_W2[n_ids] / (1 - beta1**t_step)\n",
    "        v_hat_W2_neg = v_W2[n_ids] / (1 - beta2**t_step)\n",
    "        \n",
    "        W2_neg_update = -learning_rate * m_hat_W2_neg / (np.sqrt(v_hat_W2_neg) + epsilon)\n",
    "        np.add.at(W2, n_ids, W2_neg_update)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {total_loss/len(training_pairs):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, top_n=5):\n",
    "    if word not in vocab: return \"Reč nije u rečniku.\"\n",
    "    v = W1[vocab[word]]\n",
    "    # Računamo Cosine Similarity između izabrane reči i celog W1\n",
    "    norm_W1 = np.linalg.norm(W1, axis=1)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarities = np.dot(W1, v) / (norm_W1 * norm_v + 1e-10)\n",
    "    \n",
    "    best_indices = similarities.argsort()[::-1][1:top_n+1]\n",
    "    return [inverse_vocab[i] for i in best_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea149226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REZULTATI TESTIRANJA ---\n",
      "KING: ['wound', 'hmnzs', 'phthia', 'kings', 'heroes']\n",
      "EMPIRE: Reč nije u rečniku.\n",
      "FRANCE: ['rudolf', 'worker', 'warns', 'anarchists', 'wollstonecraft']\n",
      "GERMANY: ['emile', 'recognition', 'autonomous', 'capitalist', 'communes']\n",
      "ENGLAND: ['spring', 'odd', 'reviews', 'positive', 'acre']\n",
      "COUNTRY: ['arab', 'naye', 'emirates', 'currency', 'shaikh']\n",
      "CROWN: Reč nije u rečniku.\n",
      "WATER: ['keeps', 'surface', 'fairbanks', 'amount', 'hot']\n",
      "WET: ['fire', 'divine', 'hot', 'cold', 'neoptolemus']\n",
      "DRY: ['hot', 'which', 'keeps', 'action', 'air']\n",
      "DEATH: ['doren', 'sie', 'sanctuary', 'resources', 'birthplace']\n"
     ]
    }
   ],
   "source": [
    "test_words = [\n",
    "    'king',      # Klasičan primer za Word2Vec\n",
    "    'empire',    # Česta reč u istorijskim opisima\n",
    "    'france',\n",
    "    'germany',\n",
    "    'england',\n",
    "    'country',\n",
    "    'crown',\n",
    "    'water',\n",
    "    'wet',\n",
    "    'dry',\n",
    "    'death'\n",
    "]\n",
    "\n",
    "print(\"--- REZULTATI TESTIRANJA ---\")\n",
    "for word in test_words:\n",
    "    print(f\"{word.upper()}: {most_similar(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff09464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_words(model_matrix, words_to_plot):\n",
    "    # 1. Izvlačimo vektore za odabrane reči\n",
    "    word_vectors = []\n",
    "    valid_words = []\n",
    "    \n",
    "    for word in words_to_plot:\n",
    "        if word in vocab:\n",
    "            word_vectors.append(W1[vocab[word]])\n",
    "            valid_words.append(word)\n",
    "    \n",
    "    word_vectors = np.array(word_vectors)\n",
    "    \n",
    "    # 2. PCA redukcija sa 50 na 2 dimenzije\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    # 3. Crtanje\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(result[:, 0], result[:, 1], edgecolors='k', c='red')\n",
    "    \n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), size=12, xytext=(5, 2), \n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "    \n",
    "    plt.title(\"PCA Vizuelizacija Word Embedding-a\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Pozovi funkciju sa tvojim test rečima\n",
    "plot_words(W1, test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e7b25",
   "metadata": {},
   "source": [
    "FORWARD PASS    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01219b3f",
   "metadata": {},
   "source": [
    "def softmax(x):\n",
    "    e = np.exp(x)\n",
    "    return e / e.sum()\n",
    "\n",
    "def forward_pass(word_id, W1, W2):\n",
    "    # h: embedding za word (hidden layer)\n",
    "    h = W1[word_id]\n",
    "    # u: dot product (output)\n",
    "    u = np.dot(h,W2)\n",
    "\n",
    "    y_pred = softmax(u)\n",
    "\n",
    "    return y_pred, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edf7ff",
   "metadata": {},
   "source": [
    "BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7afb378",
   "metadata": {},
   "source": [
    "def backpropagation(error, h ,word_id, W1, W2, learning_rate):\n",
    "    # gradijent za W2 (dimenzije cele matrice)\n",
    "    dW2 = np.outer(h, error)\n",
    "    # gradijent za W1 (dimenzije jednog vektora(reda) u matrici)\n",
    "    dW1 = np.dot(error, W2.T)\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    W1[word_id] -= learning_rate * dW1\n",
    "\n",
    "    return W1, W2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7ecb8",
   "metadata": {},
   "source": [
    "Glavna petlja   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02feb992",
   "metadata": {},
   "source": [
    "W1_copy = W1.copy()\n",
    "W2_copy = W2.copy()\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.2\n",
    "batch_size = 32\n",
    "vocab_size = len(vocab)\n",
    "vector_size = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    np.random.shuffle(training_pairs)\n",
    "\n",
    "    for i in range(0, len(training_pairs), batch_size):\n",
    "        batch = training_pairs[i:i + batch_size]\n",
    "        dW1_batch = np.zeros_like(W1_copy)\n",
    "        dW2_batch = np.zeros_like(W2_copy)\n",
    "        \n",
    "        for center_word_id, pair_word_id in batch:\n",
    "            y_pred, h = forward_pass(center_word_id, W1_copy, W2_copy)\n",
    "\n",
    "            error = np.copy(y_pred)\n",
    "            error[pair_word_id] -= 1\n",
    "\n",
    "            dW2 = np.outer(h, error)\n",
    "            dW1 = np.dot(error, W2_copy.T)\n",
    "\n",
    "            dW2_batch += dW2\n",
    "            dW1_batch[center_word_id] += dW1\n",
    "\n",
    "            loss += -np.log(y_pred[pair_word_id] + 1e-10)\n",
    "\n",
    "        W1_copy -= (learning_rate / len(batch)) * dW1_batch\n",
    "        W2_copy -= (learning_rate / len(batch)) * dW2_batch\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f28ddb",
   "metadata": {},
   "source": [
    "def softmax_batch(x):\n",
    "    # x je matrica dimenzija (batch_size, vocab_size)\n",
    "    \n",
    "    # 2. Oduzimamo max i računamo eksponent\n",
    "    exps = np.exp(x)\n",
    "    \n",
    "    # 3. Delimo svaki red sa sumom tog istog reda\n",
    "    sum_per_row = np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    return exps / sum_per_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0754f18",
   "metadata": {},
   "source": [
    "def forward_pass_batch(word_ids, W1, W2):\n",
    "    # h više nije (10,), sada je (batch_size, 10)\n",
    "    # NumPy automatski izvlači sve tražene redove iz W1 odjednom\n",
    "    h = W1[word_ids]\n",
    "    \n",
    "    # u više nije (vocab_size,), sada je (batch_size, vocab_size)\n",
    "    u = np.dot(h, W2)\n",
    "    \n",
    "    # Softmax mora da se izračuna za svaki red posebno\n",
    "    y_pred = softmax_batch(u)\n",
    "    \n",
    "    return y_pred, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c8355",
   "metadata": {},
   "source": [
    "def backpropagation_batch(error, h, W2, learning_rate):\n",
    "    # dW2 se računa kao dot product, što automatski sumira gradijente za ceo batch\n",
    "    # (vector_size, batch_size) @ (batch_size, vocab_size) -> (vector_size, vocab_size)\n",
    "    dW2 = np.dot(h.T, error)\n",
    "    \n",
    "    # dW1 računa koliko svaki embedding u W1 treba da se promeni\n",
    "    dW1 = np.dot(error, W2.T)\n",
    "    \n",
    "    return dW1, dW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804f9fe",
   "metadata": {},
   "source": [
    "W1_copy = W1.copy()\n",
    "W2_copy = W2.copy()\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 256\n",
    "pairs = np.array(training_pairs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    np.random.shuffle(pairs)\n",
    "\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i : i + batch_size]\n",
    "        curr_batch = len(batch)\n",
    "        c_ids, t_ids = batch[:, 0], batch[:, 1]\n",
    "\n",
    "        # 1. Forward\n",
    "        y_pred, h = forward_pass_batch(c_ids, W1_copy, W2_copy)\n",
    "\n",
    "        # 2. Loss (vektorizovano uzimanje tačnih verovatnoća)\n",
    "        loss += -np.sum(np.log(y_pred[np.arange(curr_batch), t_ids] + 1e-10))\n",
    "\n",
    "        # 3. Error i Gradijenti\n",
    "        error = y_pred\n",
    "        error[np.arange(curr_batch), t_ids] -= 1\n",
    "\n",
    "        dW2 = np.dot(h.T, error)\n",
    "        dW1 = np.dot(error, W2_copy.T)\n",
    "\n",
    "        # 4. Update\n",
    "        W2_copy -= (learning_rate / curr_batch) * dW2\n",
    "        np.add.at(W1_copy, c_ids, -(learning_rate / curr_batch) * dW1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
