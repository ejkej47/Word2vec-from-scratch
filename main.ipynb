{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec25680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8ceb0",
   "metadata": {},
   "source": [
    "Ucitavanje dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('text8') as f:\n",
    "    all_words = f.read().split()\n",
    "    sample = all_words[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa842e2",
   "metadata": {},
   "source": [
    "Uzimanje privh 1000 reci kao sample, \n",
    "Pravljenje vokabulara i njegovog inverza.\n",
    "{'anarchism': 0, 'originated': 1...}\n",
    "{0: 'anarchism', 1: 'originated'...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "count = collections.Counter(sample)\n",
    "\n",
    "sorted_count = count.most_common()\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "inverse_vocab = {}\n",
    "\n",
    "for word, _ in sorted_count:\n",
    "    vocab[word] = len(vocab)\n",
    "    inverse_vocab[len(inverse_vocab)] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a15afa",
   "metadata": {},
   "source": [
    "Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afdc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "t = 1e-4  # Threshold \n",
    "total_count = len(sample)\n",
    "word_freqs = {word: count[word] / total_count for word in vocab}\n",
    "\n",
    "subsampled_sample = []\n",
    "\n",
    "for word in sample:\n",
    "    f_wi = word_freqs[word]\n",
    "    # P(wi) = 1 - sqrt(t / f(wi))\n",
    "    p_discard = max(0, 1 - np.sqrt(t / f_wi))\n",
    "    \n",
    "    # Ako je nasumicni broj veci od verovatnoee odbacivanja, zadrzavamo rec\n",
    "    if random.random() > p_discard:\n",
    "        subsampled_sample.append(word)\n",
    "\n",
    "print(f\"Originalna dužina: {len(sample)}\")\n",
    "print(f\"Nakon subsamplinga: {len(subsampled_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579ac47",
   "metadata": {},
   "source": [
    "Pravljenje windowa(indeksiranje, tj. pravljenje parova po ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c666e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "training_pairs = []\n",
    "\n",
    "for i in range(len(subsampled_sample)):\n",
    "    #uzimamo id reci\n",
    "    center_word_id = vocab[subsampled_sample[i]]\n",
    "\n",
    "    start = max(0, i - window_size)\n",
    "    end = min(i + window_size + 1, len(subsampled_sample))\n",
    "\n",
    "    for j in range(start, end):\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        context_word_id = vocab[subsampled_sample[j]]\n",
    "        training_pairs.append((center_word_id, context_word_id))\n",
    "\n",
    "training_pairs = np.array(training_pairs)\n",
    "\n",
    "print(f\"Ukupno parova: {len(training_pairs)}\")\n",
    "print(f\"Prvih 20 parova (ID, ID): {training_pairs[:20]}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83ac2f",
   "metadata": {},
   "source": [
    "Pravljenje matrica w1 i w2, i postavljanje limita putem xavier inicijalizacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "vector_size = 30      \n",
    "batch_size = 1024      \n",
    "epochs = 20\n",
    "learning_rate = 0.02\n",
    "num_neg_samples = 10 \n",
    "\n",
    "limit = np.sqrt(6 / (vocab_size + vector_size))\n",
    "\n",
    "W1 = np.random.uniform(-limit, limit, (vocab_size, vector_size))\n",
    "W2 = np.random.uniform(-limit, limit, (vocab_size, vector_size))\n",
    "\n",
    "\n",
    "print(limit)\n",
    "print(f\"W1 (Vokabular x Vektor): {W1.shape}\")\n",
    "print(f\"W2 (Vektor x Vokabular): {W2.shape}\")\n",
    "\n",
    "word_counts_arr = np.array([count[inverse_vocab[i]] for i in range(vocab_size)])\n",
    "probs = word_counts_arr**0.75\n",
    "probs /= probs.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bf50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -20, 20)))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(training_pairs)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, len(training_pairs), batch_size):\n",
    "        batch = training_pairs[i : i + batch_size]\n",
    "        curr_batch = len(batch)\n",
    "        \n",
    "        c_ids = batch[:, 0] # Centralne reči\n",
    "        t_ids = batch[:, 1] # Target (pozitivne) reči\n",
    "        \n",
    "        # Nasumično biramo netačne reči za ceo batch odjednom\n",
    "        n_ids = np.random.choice(vocab_size, size=(curr_batch, num_neg_samples), p=probs)\n",
    "        \n",
    "        # --- FORWARD ---\n",
    "        h = W1[c_ids]            # (batch, vector)\n",
    "        v_pos = W2[t_ids]        # (batch, vector)\n",
    "        v_neg = W2[n_ids]        # (batch, neg, vector)\n",
    "        \n",
    "        # Skorovi (Log-Sigmoid)\n",
    "        pos_dot = np.sum(h * v_pos, axis=1)\n",
    "        pos_probs = sigmoid(pos_dot)\n",
    "        \n",
    "        neg_dot = np.einsum('ij,ikj->ik', h, v_neg) # Batch dot product\n",
    "        neg_probs = sigmoid(-neg_dot)\n",
    "        \n",
    "        # Loss (Binary Cross Entropy)\n",
    "        loss = -np.sum(np.log(pos_probs + 1e-10)) - np.sum(np.log(neg_probs + 1e-10))\n",
    "        total_loss += loss\n",
    "        \n",
    "        # --- BACKPROP (Gradijenti) ---\n",
    "        grad_pos = (pos_probs - 1).reshape(-1, 1) # (batch, 1)\n",
    "        grad_neg = (1 - neg_probs)                # (batch, neg)\n",
    "        \n",
    "        # Gradijenti za matrice\n",
    "        dW2_pos = grad_pos * h\n",
    "        dW2_neg = np.einsum('ik,ij->ikj', grad_neg, h)\n",
    "        \n",
    "        dW1 = grad_pos * v_pos + np.einsum('ik,ikj->ij', grad_neg, v_neg)\n",
    "        \n",
    "        # --- UPDATE (np.add.at sprečava gubitak podataka kod duplikata) ---\n",
    "        np.add.at(W1, c_ids, -learning_rate * dW1)\n",
    "        np.add.at(W2, t_ids, -learning_rate * dW2_pos)\n",
    "        np.add.at(W2, n_ids, -learning_rate * dW2_neg)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {total_loss/len(training_pairs):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, top_n=5):\n",
    "    if word not in vocab: return \"Reč nije u rečniku.\"\n",
    "    v = W1[vocab[word]]\n",
    "    # Računamo Cosine Similarity između izabrane reči i celog W1\n",
    "    norm_W1 = np.linalg.norm(W1, axis=1)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarities = np.dot(W1, v) / (norm_W1 * norm_v + 1e-10)\n",
    "    \n",
    "    best_indices = similarities.argsort()[::-1][1:top_n+1]\n",
    "    return [inverse_vocab[i] for i in best_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea149226",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    'king',      # Klasičan primer za Word2Vec\n",
    "    'empire',    # Česta reč u istorijskim opisima\n",
    "    'france',\n",
    "    'germany',\n",
    "    'england',\n",
    "    'country',\n",
    "    'crown',\n",
    "    'water',\n",
    "    'wet',\n",
    "    'dry',\n",
    "    'death'\n",
    "]\n",
    "\n",
    "print(\"--- REZULTATI TESTIRANJA ---\")\n",
    "for word in test_words:\n",
    "    print(f\"{word.upper()}: {most_similar(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff09464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_words(model_matrix, words_to_plot):\n",
    "    # 1. Izvlačimo vektore za odabrane reči\n",
    "    word_vectors = []\n",
    "    valid_words = []\n",
    "    \n",
    "    for word in words_to_plot:\n",
    "        if word in vocab:\n",
    "            word_vectors.append(W1[vocab[word]])\n",
    "            valid_words.append(word)\n",
    "    \n",
    "    word_vectors = np.array(word_vectors)\n",
    "    \n",
    "    # 2. PCA redukcija sa 50 na 2 dimenzije\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    # 3. Crtanje\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(result[:, 0], result[:, 1], edgecolors='k', c='red')\n",
    "    \n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), size=12, xytext=(5, 2), \n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "    \n",
    "    plt.title(\"PCA Vizuelizacija Word Embedding-a\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Pozovi funkciju sa tvojim test rečima\n",
    "plot_words(W1, test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e7b25",
   "metadata": {},
   "source": [
    "FORWARD PASS    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01219b3f",
   "metadata": {},
   "source": [
    "def softmax(x):\n",
    "    e = np.exp(x)\n",
    "    return e / e.sum()\n",
    "\n",
    "def forward_pass(word_id, W1, W2):\n",
    "    # h: embedding za word (hidden layer)\n",
    "    h = W1[word_id]\n",
    "    # u: dot product (output)\n",
    "    u = np.dot(h,W2)\n",
    "\n",
    "    y_pred = softmax(u)\n",
    "\n",
    "    return y_pred, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edf7ff",
   "metadata": {},
   "source": [
    "BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7afb378",
   "metadata": {},
   "source": [
    "def backpropagation(error, h ,word_id, W1, W2, learning_rate):\n",
    "    # gradijent za W2 (dimenzije cele matrice)\n",
    "    dW2 = np.outer(h, error)\n",
    "    # gradijent za W1 (dimenzije jednog vektora(reda) u matrici)\n",
    "    dW1 = np.dot(error, W2.T)\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    W1[word_id] -= learning_rate * dW1\n",
    "\n",
    "    return W1, W2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7ecb8",
   "metadata": {},
   "source": [
    "Glavna petlja   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02feb992",
   "metadata": {},
   "source": [
    "W1_copy = W1.copy()\n",
    "W2_copy = W2.copy()\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.2\n",
    "batch_size = 32\n",
    "vocab_size = len(vocab)\n",
    "vector_size = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    np.random.shuffle(training_pairs)\n",
    "\n",
    "    for i in range(0, len(training_pairs), batch_size):\n",
    "        batch = training_pairs[i:i + batch_size]\n",
    "        dW1_batch = np.zeros_like(W1_copy)\n",
    "        dW2_batch = np.zeros_like(W2_copy)\n",
    "        \n",
    "        for center_word_id, pair_word_id in batch:\n",
    "            y_pred, h = forward_pass(center_word_id, W1_copy, W2_copy)\n",
    "\n",
    "            error = np.copy(y_pred)\n",
    "            error[pair_word_id] -= 1\n",
    "\n",
    "            dW2 = np.outer(h, error)\n",
    "            dW1 = np.dot(error, W2_copy.T)\n",
    "\n",
    "            dW2_batch += dW2\n",
    "            dW1_batch[center_word_id] += dW1\n",
    "\n",
    "            loss += -np.log(y_pred[pair_word_id] + 1e-10)\n",
    "\n",
    "        W1_copy -= (learning_rate / len(batch)) * dW1_batch\n",
    "        W2_copy -= (learning_rate / len(batch)) * dW2_batch\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f28ddb",
   "metadata": {},
   "source": [
    "def softmax_batch(x):\n",
    "    # x je matrica dimenzija (batch_size, vocab_size)\n",
    "    \n",
    "    # 2. Oduzimamo max i računamo eksponent\n",
    "    exps = np.exp(x)\n",
    "    \n",
    "    # 3. Delimo svaki red sa sumom tog istog reda\n",
    "    sum_per_row = np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    return exps / sum_per_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0754f18",
   "metadata": {},
   "source": [
    "def forward_pass_batch(word_ids, W1, W2):\n",
    "    # h više nije (10,), sada je (batch_size, 10)\n",
    "    # NumPy automatski izvlači sve tražene redove iz W1 odjednom\n",
    "    h = W1[word_ids]\n",
    "    \n",
    "    # u više nije (vocab_size,), sada je (batch_size, vocab_size)\n",
    "    u = np.dot(h, W2)\n",
    "    \n",
    "    # Softmax mora da se izračuna za svaki red posebno\n",
    "    y_pred = softmax_batch(u)\n",
    "    \n",
    "    return y_pred, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c8355",
   "metadata": {},
   "source": [
    "def backpropagation_batch(error, h, W2, learning_rate):\n",
    "    # dW2 se računa kao dot product, što automatski sumira gradijente za ceo batch\n",
    "    # (vector_size, batch_size) @ (batch_size, vocab_size) -> (vector_size, vocab_size)\n",
    "    dW2 = np.dot(h.T, error)\n",
    "    \n",
    "    # dW1 računa koliko svaki embedding u W1 treba da se promeni\n",
    "    dW1 = np.dot(error, W2.T)\n",
    "    \n",
    "    return dW1, dW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804f9fe",
   "metadata": {},
   "source": [
    "W1_copy = W1.copy()\n",
    "W2_copy = W2.copy()\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 256\n",
    "pairs = np.array(training_pairs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    np.random.shuffle(pairs)\n",
    "\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i : i + batch_size]\n",
    "        curr_batch = len(batch)\n",
    "        c_ids, t_ids = batch[:, 0], batch[:, 1]\n",
    "\n",
    "        # 1. Forward\n",
    "        y_pred, h = forward_pass_batch(c_ids, W1_copy, W2_copy)\n",
    "\n",
    "        # 2. Loss (vektorizovano uzimanje tačnih verovatnoća)\n",
    "        loss += -np.sum(np.log(y_pred[np.arange(curr_batch), t_ids] + 1e-10))\n",
    "\n",
    "        # 3. Error i Gradijenti\n",
    "        error = y_pred\n",
    "        error[np.arange(curr_batch), t_ids] -= 1\n",
    "\n",
    "        dW2 = np.dot(h.T, error)\n",
    "        dW1 = np.dot(error, W2_copy.T)\n",
    "\n",
    "        # 4. Update\n",
    "        W2_copy -= (learning_rate / curr_batch) * dW2\n",
    "        np.add.at(W1_copy, c_ids, -(learning_rate / curr_batch) * dW1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
